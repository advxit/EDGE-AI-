# Temporal Trade-offs in Spiking Neural Networks for Edge-Aware Neuromorphic Systems

## ğŸ“Œ Overview

This project investigates the design, training, optimization, and deployment feasibility of **Leaky Integrate-and-Fire (LIF) based Spiking Neural Networks (SNNs)** with a focus on **temporal performance vs hardware efficiency trade-offs**.

Spiking Neural Networks operate using discrete spikes and temporal membrane dynamics, making them highly suitable for **neuromorphic computing, edge AI, and hardware-aware machine learning systems**.

This repository implements an end-to-end pipeline â€” from SNN training to edge deployment formats â€” while analyzing implications from **VLSI and analog neuromorphic perspectives**.

---

## ğŸ¯ Objectives

* Implement LIF-based SNN architectures
* Train SNNs using surrogate gradient learning
* Evaluate timestep-dependent performance trade-offs
* Analyze spike sparsity as an energy proxy
* Study hardware-aware compute implications
* Export trained models to deployment formats
* Explore neuromorphic hardware mapping feasibility

---

## ğŸ§  Core Neuromorphic Concepts

* Membrane integration & leakage dynamics
* Spike-driven event computation
* Temporal evidence accumulation
* Surrogate gradient backpropagation
* Sparse activation patterns
* Energy-efficient inference

---

## ğŸ—ï¸ Model Architecture

Baseline pipeline:

Input â†’ Fully Connected Layer â†’ LIF Neuron â†’ Fully Connected Layer â†’ LIF Output Layer

Key configurable parameters:

* Simulation timesteps (T)
* Membrane decay constant (Î²)
* Threshold dynamics
* Surrogate gradient function
* Temporal batch normalization

---

## ğŸ“Š Experimental Focus

Primary research axis:

```
T âˆˆ {5, 10, 25, 50}
```

For each configuration we measure:

* Classification Accuracy
* Inference Latency
* Total Spike Count
* Average Firing Rate
* Spike Sparsity

These metrics allow evaluation of **accuracy vs efficiency trade-offs**.

---

## âš¡ Hardware & VLSI-Aware Analysis

To bridge algorithm design with hardware feasibility:

* MAC vs AC operation comparison
* Memory access estimation
* Temporal compute overhead
* Spike-driven sparsity analysis

This provides insights into neuromorphic hardware efficiency without requiring full RTL implementation.

---

## ğŸ”Œ Analog Neuromorphic Interpretation

LIF neuron behavior maps to analog primitives:

| SNN Component      | Analog Equivalent           |
| ------------------ | --------------------------- |
| Membrane Potential | Capacitor Voltage           |
| Synaptic Input     | Current Injection           |
| Leakage            | Resistive/Subthreshold Leak |
| Thresholding       | Comparator                  |
| Spike              | Digital Pulse               |
| Reset              | Capacitor Discharge         |

Temporal spike sparsity implies reduced switching activity and analog energy consumption.

---

## ğŸ› ï¸ Tech Stack

* PyTorch
* snnTorch
* SpikingJelly
* CUDA (GPU training)
* NumPy
* Matplotlib
* ONNX
* TensorFlow Lite

---

## ğŸ§ª Training Methodology

Training incorporates:

* Surrogate gradient learning
* Adam optimizer
* Temporal spike accumulation
* Backpropagation through time

Optional enhancements:

* BatchNorm Through Time (BNTT)
* Spike regularization
* Threshold tuning

---

## ğŸ”„ Deployment Pipeline

```
PyTorch SNN
     â†“
ONNX Export
     â†“
TensorFlow Lite Conversion
     â†“
Edge Inference Evaluation
```

This enables compatibility testing with lightweight inference environments.

---

## ğŸ§­ Neuromorphic Hardware Outlook

Future hardware mapping targets include:

* Intel Loihi
* FPGA-based neuromorphic accelerators
* Mixed-signal analog SNN implementations

These explorations focus on translating spike-driven models to hardware substrates.

---

## ğŸ“¡ Event-Based Vision Extension

Planned dataset expansion:

* DVS Gesture Dataset
* Event-driven temporal encoding
* Frame vs event batching comparison

This will enable evaluation on true neuromorphic sensory data.

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ lif_basics/              # Single neuron simulations
â”œâ”€â”€ snn_models/              # Network architectures
â”œâ”€â”€ surrogate_training/     # Gradient-based learning
â”œâ”€â”€ experiments/             # Timestep trade-off studies
â”œâ”€â”€ metrics/                 # Spike & latency logging
â”œâ”€â”€ deployment/              # ONNX & TFLite export
â”œâ”€â”€ hardware_analysis/       # VLSI & analog evaluation
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Project Status

* âœ… LIF neuron modeling
* âœ… SNN architecture implementation
* â³ Surrogate gradient training
* â³ Temporal trade-off experiments
* â³ Hardware-aware analysis
* â³ Deployment benchmarking
* â³ DVS dataset integration

Estimated completion: ~70%

---

## ğŸ¤ Future Extensions

* SpikingJelly comparative frameworks
* Quantization-aware SNN training
* Mixed-signal neuromorphic mapping
* Event-driven real-time inference
* Edge deployment benchmarking

---

## ğŸ“š References

Key inspirations include:

* Neuromorphic computing literature
* Surrogate gradient SNN training
* Edge AI deployment workflows
* Hardware-aware neural network design

Full citations will accompany the research publication.

---

## ğŸ‘¨â€ğŸ’» Author

Advait Rao
Electronics & Engineering
Neuromorphic Systems â€¢ Edge AI â€¢ Hardware-Aware AI

---

## ğŸ“œ License

Released under the MIT License.
